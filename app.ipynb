{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os \n",
    "from tortoise.models.classifier import AudioMiniEncoderWithClassifierHead\n",
    "from glob import glob\n",
    "import io\n",
    "import librosa\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audiopath, sampling_rate=22000):\n",
    "    if isinstance(audiopath, str):\n",
    "        if audiopath.endswith('.mp3'):\n",
    "            audio, lsr= librosa.load(audiopath,sr=sampling_rate)\n",
    "            audio = torch.FloatTensor(audio)\n",
    "        else:\n",
    "            assert False, f'Unsupported audio format provided: {audiopath[-4]}'\n",
    "    elif isinstance(audiopath,io.BytesIO):\n",
    "        audio,lsr = torchaudio.load(audiopath)    \n",
    "        audio = audio[0]\n",
    "    \n",
    "    if lsr != sampling_rate:\n",
    "        audio = torchaudio.functional.resample(audio,lsr,sampling_rate)\n",
    "\n",
    "    if torch.any(audio > 2) or not torch.any(audio < 0):\n",
    "        print(f'Error with audio data. Max = {audio.max()} min ={audio.min()}')\n",
    "    audio.clip_(-1,1)\n",
    "\n",
    "    return audio.unsqueeze(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_audio_clip(clip):\n",
    "    classifer = AudioMiniEncoderWithClassifierHead(2, spec_dim=1,embedding_dim = 512, depth = 5,\n",
    "                                                    downsample_factor = 4, resnet_blocks=2 ,\n",
    "                                                    attn_blocks = 4, num_attn_heads = 4, \n",
    "                                                    base_channels = 32, dropout = 0 , kernel_size = 5 , distribute_zero_label= False)\n",
    "    \n",
    "    state_dict = torch.load('tortoise/data/mel_norms.pth',map_location=torch.device('cpu'))\n",
    "    # print(type(state_dict))\n",
    "    classifer.load_state_dict(state_dict)\n",
    "    clip = clip.cpu().unsqueeze(0)\n",
    "    results = F.softmax(classifer(clip),dim = -1)\n",
    "    return type(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'torch.Tensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclassify_audio_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexamples/favorite_riding_hood.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mclassify_audio_clip\u001b[0;34m(clip)\u001b[0m\n\u001b[1;32m      7\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtortoise/data/mel_norms.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(type(state_dict))\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mclassifer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m clip \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m results \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(classifer(clip),dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2140\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[1;32m   2106\u001b[0m \n\u001b[1;32m   2107\u001b[0m \u001b[38;5;124;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2137\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   2138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 2140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2142\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2143\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'torch.Tensor'>."
     ]
    }
   ],
   "source": [
    "classify_audio_clip(\"examples/favorite_riding_hood.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Page itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.set_page_config(layout=\"wide\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 18:52:43.396 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/safsof/.local/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    st.title('AI Generated Audio Detector')\n",
    "    uploaded_file = st.file_uploader('Upload an Audio file',type=['mp3'])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        if st.button('Analyze Audio'):\n",
    "            col1,col2,col3 = st.columns(3)\n",
    "\n",
    "            with col1:\n",
    "                st.info(\"Your Results are below\")\n",
    "                audio_clip = load_audio(uploaded_file)\n",
    "                result = classify_audio_clip(audio_clip)\n",
    "                result = result.item()\n",
    "                st.info(f\"Results Probability: {result}\")\n",
    "                st.success(f'the uploaded audio is {result *100:.2f}% likely to be AI generated')\n",
    "            \n",
    "            with col2:\n",
    "                st.info('Your uploaded audio is below')\n",
    "                st.audio(uploaded_file)\n",
    "\n",
    "                fig = px.line()\n",
    "                fig.add_scatter(x=list(range(len(audio_clip.squeeze()))),y=audio_clip.squeeze())\n",
    "                fig.update_layout(\n",
    "                    title = 'Waveform Plot',\n",
    "                    xaxis_title = 'Time',\n",
    "                    yaxis_title = 'Amplitude',\n",
    "                )\n",
    "\n",
    "                st.plotly_chart(fig,use_container_width=True)\n",
    "            \n",
    "            with col3:\n",
    "                st.info(\"Disclaimer\")\n",
    "                st.warning(\"These are for a university project, and we are still learning so don't take our accuracy for granted, we could be wrong so idk check multiple times\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
